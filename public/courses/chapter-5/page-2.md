# 5.2. Vấn đề cốt lõi: AI có thể "ảo giác" (Hallucination)

![AI Hallucination Warning](/images/chapter-5/5.2-ai-hallucination.svg)

## ⚠️ **Hiện tượng "ảo giác" AI**

### **"Ảo giác" là gì?**

- Khi AI tự tin bịa ra thông tin không có thật
- Xảy ra vì AI chỉ dựa vào xác suất thống kê, không có nhận thức về đúng/sai
- **Ví dụ:** AI có thể "chế" ra một nghiên cứu không tồn tại hoặc một công dụng không có thật của thảo dược

### **⚡ Tại sao điều này nguy hiểm?**

- AI trả lời với vẻ rất tự tin, khiến người dùng dễ tin
- Thông tin sai có thể dẫn đến chẩn đoán hoặc điều trị không chính xác
- **Đây là lý do bạn không bao giờ được tin tưởng AI 100%**

## 🔍 **Case study: Bài thuốc không tồn tại**

### **Prompt nguy hiểm:**

```
"Gợi ý một bài thuốc cổ phương ít người biết để trị chứng Can khí uất kết."
```

### **AI có thể trả lời:**

_"Bạn có thể dùng bài 'An Thần Giải Uất Thang' trong sách 'Thảo Dược Bí Truyền'."_

### **⚠️ Vấn đề:**

Có thể cả bài thuốc và cuốn sách đó đều do AI tự bịa ra!

### **✅ Cách xử lý đúng:**

Tra cứu tên bài thuốc và tên sách trên các nguồn thư viện y học uy tín để xác thực.

## 🛡️ **Nguyên tắc phòng ngừa**

### **1. Luôn nghi ngờ thông tin mới lạ**

- Nếu AI đưa ra thông tin bạn chưa từng nghe, hãy kiểm tra kỹ
- Đặc biệt cảnh giác với tên bài thuốc, tên sách hiếm

### **2. Ưu tiên nguồn đáng tin cậy**

- Sách giáo khoa y học cổ truyền
- Tạp chí chuyên ngành có uy tín
- Website của các trường đại học y khoa

### **3. So sánh với kinh nghiệm**

- Thông tin AI có hợp lý với kiến thức nền không?
- Có mâu thuẫn với kinh nghiệm lâm sàng không?
